{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input, n_neurons, activation = None, weights = None, bias = None):\n",
    "        \"\"\"\n",
    "        :param int n_input: input nodes\n",
    "        :param int n_neurons: output nodes\n",
    "        :param str activation: activation function\n",
    "        :param weights: weight vectors\n",
    "        :param bias: bias vectors\n",
    "        \"\"\"\n",
    "        self.weights = weights if weights is not None else np.random.rand(n_input, n_neurons)* np.sqrt(1/n_neurons)\n",
    "        self.bias = bias if bias is not None else np.random.rand(n_neurons)\n",
    "        self.activation = activation\n",
    "        self.last_activation = None\n",
    "        self.error = None\n",
    "        self.delta = None \n",
    "    \n",
    "    def activate(self, x):\n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        self.last_activation = self._apply_activation(r)\n",
    "        return self.last_activation\n",
    "\n",
    "    def _apply_activation(self, r):\n",
    "        if self.activation == None:\n",
    "            return r #no activation function\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(r,0)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-r))\n",
    "        return r\n",
    "    def apply_activation_derivatives(self, r):\n",
    "        if self.activation is None:\n",
    "            return np.ones_like(r)\n",
    "        elif self.activation == 'Relu':\n",
    "            grad = np.array(r, copy=True)\n",
    "            grad[r > 0] = 1.\n",
    "            grad[r <= 0] = 0.\n",
    "            return grad\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return r*(1-r)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # Neural Network Class\n",
    "    def __init__(self):\n",
    "        self._layers = []  # list of network class\n",
    "    def add_layer(self, layer):\n",
    "        # Add layers\n",
    "        self._layers.append(layer)\n",
    "    def feed_forward(self, X):\n",
    "        # Forward calculation\n",
    "        for layer in self._layers:\n",
    "            # Loop through every layer\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "          # Back propagation\n",
    "        # Get result of forward calculation\n",
    "        output = self.feed_forward(X)\n",
    "        for i in reversed(range(len(self._layers))):\n",
    "            layer = self._layers[i]\n",
    "            # If itâ€™s output layer\n",
    "            if layer == self._layers[-1]:\n",
    "                layer.error = y - output\n",
    "                # calculate delta\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(output)\n",
    "            else:\n",
    "                next_layer = self._layers[i + 1]\n",
    "                layer.error = np.dot(next_layer.weights,next_layer.delta)\n",
    "                 # Calculate delta\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(layer.last_activation)\n",
    "        for i in range(len(self._layers)):\n",
    "            layer = self._layers[i]\n",
    "                      # o_i is output of previous layer\n",
    "            o_i = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)\n",
    "                      # Gradient descent\n",
    "            layer.weights += layer.delta * o_i.T * learning_rate\n",
    "    def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):\n",
    "        # Train network\n",
    "        # one-hot encoding\n",
    "        y_onehot = np.zeros((y_train.shape[0], 2))\n",
    "        y_onehot[np.arange(y_train.shape[0]), y_train] = 1\n",
    "        mses = []\n",
    "        for i in range(max_epochs):\n",
    "            for j in range(len(X_train)):\n",
    "                self.backpropagation(X_train[j], y_onehot[j],learning_rate)\n",
    "            if i % 10 == 0:\n",
    "                mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))\n",
    "            mses.append(mse)\n",
    "            print('Epoch: #%s, MSE: %f' % (i, float(mse)))\n",
    "            print('Accuracy: %.2f%%' % (self.accuracy(self.predict(X_test), y_test.flatten()) * 100))\n",
    "        return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer(Layer(2, 25, 'sigmoid'))  # Hidden layer 1, 2=>25\n",
    "nn.add_layer(Layer(25, 50, 'sigmoid')) # Hidden layer 2, 25=>50\n",
    "nn.add_layer(Layer(50, 25, 'sigmoid')) # Hidden layer 3, 50=>25\n",
    "nn.add_layer(Layer(25, 2, 'sigmoid'))  # Hidden layer, 25=>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
